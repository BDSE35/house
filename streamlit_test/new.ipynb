{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_mean shape: (1, 1, 16)\n",
      "x_std shape: (1, 1, 16)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "# 设置字体为 Microsoft YaHei（微軟正黑體）\n",
    "rcParams[\"font.sans-serif\"] = [\"Microsoft YaHei\"]\n",
    "rcParams[\"axes.unicode_minus\"] = False  # 解决负号显示问题\n",
    "\n",
    "# %%\n",
    "def create_time_window_sequences(\n",
    "    numerical_features, one_hot_features, target, window_size, step=1\n",
    "):\n",
    "    sequences, targets = [], []\n",
    "\n",
    "    # 提取每坪單價（假設這是 numerical_features 的第一列）\n",
    "    unit_price = numerical_features[:, 0].reshape(-1, 1)  # 每坪單價（目標變數的一部分）\n",
    "    other_features = numerical_features[:, 1:]  # 其他數值特徵\n",
    "\n",
    "    # 逐步創建時間窗口\n",
    "    for i in range(0, len(other_features) - window_size, step):\n",
    "        # 提取過去 window_size 筆的數值特徵和歷史房價\n",
    "        num_features_window = other_features[i : i + window_size]\n",
    "        unit_price_window = unit_price[i : i + window_size]\n",
    "\n",
    "        # 提取對應的 One-Hot 特徵（前 30 筆）\n",
    "        one_hot_window = one_hot_features[i : i + window_size]\n",
    "\n",
    "        # 將數值特徵、房價和 One-Hot 特徵拼接為一個窗口\n",
    "        combined_window = np.hstack(\n",
    "            [num_features_window, unit_price_window, one_hot_window]\n",
    "        )\n",
    "\n",
    "        # 提取第 31 筆資料的特徵\n",
    "        next_num_feature = other_features[i + window_size]\n",
    "        # next_unit_price = unit_price[i + window_size]\n",
    "        next_one_hot_feature = one_hot_features[i + window_size]\n",
    "        # 初始化第31天房價=0\n",
    "        next_unit_price = 0\n",
    "\n",
    "        # 拼接第 31 筆的特徵\n",
    "        next_combined = np.hstack(\n",
    "            [next_num_feature, next_unit_price, next_one_hot_feature]\n",
    "        )\n",
    "\n",
    "        # 將第 31 筆資料與前 30 筆資料拼接\n",
    "        full_window = np.vstack([combined_window, next_combined])\n",
    "\n",
    "        # 將拼接後的完整窗口添加到序列\n",
    "        sequences.append(full_window)\n",
    "\n",
    "        # 將第 31 筆的房價作為目標\n",
    "        targets.append(target[i + window_size])\n",
    "\n",
    "    # 將序列和目標轉換為 NumPy 陣列\n",
    "    sequences = np.array(sequences)  # [num_samples, seq_length + 1, num_features]\n",
    "    targets = np.array(targets)\n",
    "\n",
    "    return sequences, targets\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # === 定義全連接層（前置 Dense 層）===\n",
    "        # self.pre_dense1 = nn.Linear(input_dim, 128)  # 將輸入維度轉為 128\n",
    "        # self.pre_dense2 = nn.Linear(128, 64)  # 將 128 維轉為 64 維\n",
    "        # self.pre_dense_activation = nn.GELU()  # 激活函數\n",
    "        # self.dropout = nn.Dropout(dropout)  # Dropout\n",
    "\n",
    "        # === 定義 LSTM 層 ===\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "\n",
    "        # === 定義後續的全連接層 ===\n",
    "        # self.fc1 = nn.Linear(hidden_dim*31, 8)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 8)\n",
    "        # self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.fc2 = nn.Linear(8, 4)\n",
    "        # self.bn2 = nn.BatchNorm1d(8)\n",
    "        self.fc3 = nn.Linear(4, output_dim)\n",
    "\n",
    "        # 激活函數\n",
    "        self.gelu = nn.GELU()\n",
    "        self.tahn = nn.Tanh()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # === 通過 Dense 層處理輸入 ===\n",
    "        # out = self.pre_dense1(x)\n",
    "        # out = self.pre_dense_activation(out)\n",
    "        # out = self.dropout(out)\n",
    "\n",
    "        # out = self.pre_dense2(out)\n",
    "        # out = self.pre_dense_activation(out)\n",
    "        # out = self.dropout(out)\n",
    "\n",
    "        # 初始化 LSTM 的隱藏狀態和細胞狀態\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # === 前向傳播 LSTM ===\n",
    "        out, _ = self.lstm(\n",
    "            x, (h0, c0)\n",
    "        )  # LSTM 層輸入形狀 (batch_size, time_steps, input_size)\n",
    "        out = out[:, -1, :]  # 取最後一個時間步的輸出\n",
    "        # out = self.flatten(out)\n",
    "        # out = out.reshape(out.size(0), -1)\n",
    "\n",
    "        # === 通過後續的全連接層 ===\n",
    "        out = self.tahn(self.fc1(out))\n",
    "        out = self.gelu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# %%\n",
    "best_params = {\n",
    "    \"hidden_dim\": 64,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.1,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"batch_size\": 8,\n",
    "}\n",
    "\n",
    "x_mean = np.array([\n",
    "    167.1636969, 3.389284115, 1.927202627, 2.585403711, 0.982602856,\n",
    "    25.57844386, 12582.38437, 0.001363799, 0.946743943, 2.385796727,\n",
    "    3.582349137, 5.061554152, 13.0768519, 20.77470275, 2018.693194, 6.699606985\n",
    "])\n",
    "\n",
    "x_std = np.array([\n",
    "    265.9473672, 1.146308572, 0.441717588, 1.178277433, 0.1307462,\n",
    "    36.91268605, 3884.340303, 0.000676091, 1.327738937, 2.735231276,\n",
    "    4.175769028, 6.34560646, 12.99347623, 20.84464324, 3.455128533, 3.394999201\n",
    "])\n",
    "\n",
    "# 重塑以便於在標準化時使用\n",
    "x_mean = x_mean[np.newaxis, np.newaxis, ...]  # [1, 1, num_features]\n",
    "x_std = x_std[np.newaxis, np.newaxis, ...]  # [1, 1, num_features]\n",
    "\n",
    "print(\"x_mean shape:\", x_mean.shape)\n",
    "print(\"x_std shape:\", x_std.shape)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_numerical_data = np.array([[\n",
    "    150.0,  # 建物移轉總面積平方公尺\n",
    "    3,      # 建物現況格局-房\n",
    "    2,      # 建物現況格局-廳\n",
    "    2,      # 建物現況格局-衛\n",
    "    1,      # 建物現況格局-隔間\n",
    "    10,     # 屋齡\n",
    "    13000,  # stockTW\n",
    "    0.0015, # KDE_1km\n",
    "    1,      # good_count_0_500\n",
    "    3,      # good_count_500_1000\n",
    "    5,      # good_count_1000_1500\n",
    "    2,      # bad_count_0_500\n",
    "    6,      # bad_count_500_1000\n",
    "    12,     # bad_count_1000_1500\n",
    "    2020,   # year\n",
    "    7       # month\n",
    "]])\n",
    "new_one_hot_data = np.array([[\n",
    "    0, 1, 0, 0,   # 建築型態: 公寓\n",
    "    1,            # 是否包含車位: 是\n",
    "    0, 1, 0, 0,   # 建材: 鋼筋\n",
    "    0, 1, 0, 0, 0,  # 鄉鎮: 下營區\n",
    "    0, 0, 0, 0, 0,  # 其他鄉鎮都設為 0\n",
    "    0, 0, 0, 0, 0,\n",
    "    0, 0, 0, 0, 0,\n",
    "    0, 0, 0, 0, 0,\n",
    "    0, 0, 0, 0, 0,\n",
    "    0, 0, 0, 0, 0,\n",
    "    0, 0, 0, 0, 0\n",
    "]])\n",
    "# Extract the numerical features to standardize (removing extra dimensions)\n",
    "numerical_part_to_standardize = new_numerical_data[:, :16].squeeze()\n",
    "\n",
    "# Standardize the numerical features\n",
    "numerical_part_to_standardize = (numerical_part_to_standardize - x_mean) / x_std\n",
    "\n",
    "# Ensure both arrays are 2D before concatenating\n",
    "numerical_part_to_standardize = numerical_part_to_standardize.reshape(1, -1)\n",
    "new_one_hot_data = new_one_hot_data.reshape(1, -1)\n",
    "\n",
    "# Concatenate the standardized numerical features and the One-Hot encoded features\n",
    "X_new = np.hstack([numerical_part_to_standardize, new_one_hot_data])\n",
    "\n",
    "# Convert NaN values to 0\n",
    "X_new = np.nan_to_num(X_new, nan=0.0)\n",
    "\n",
    "# Convert the data to a PyTorch tensor and add a batch dimension\n",
    "X_new_tensor = torch.FloatTensor(X_new).unsqueeze(0)\n",
    "\n",
    "\n",
    "y_mean = 186150.98086564965\n",
    "y_std = 77614.09986944433"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\student\\AppData\\Local\\Temp\\ipykernel_26964\\3699514861.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"LSTM_model1101.pth\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 62, got 65",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 進行預測\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 16\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_new_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m prediction\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# 將結果移回 CPU 並轉為 NumPy 陣列\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 反標準化預測結果\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\student\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\student\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 118\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    115\u001b[0m c0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# === 前向傳播 LSTM ===\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# LSTM 層輸入形狀 (batch_size, time_steps, input_size)\u001b[39;00m\n\u001b[0;32m    121\u001b[0m out \u001b[38;5;241m=\u001b[39m out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# 取最後一個時間步的輸出\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# out = self.flatten(out)\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# out = out.reshape(out.size(0), -1)\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# === 通過後續的全連接層 ===\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\student\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\student\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\student\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:913\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    910\u001b[0m             hx \u001b[38;5;241m=\u001b[39m (hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    912\u001b[0m         \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[1;32m--> 913\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    914\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\student\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:827\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    823\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[0;32m    824\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[0;32m    825\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[0;32m    826\u001b[0m                        ):\n\u001b[1;32m--> 827\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    829\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    830\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    831\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\student\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:246\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_input_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    247\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 62, got 65"
     ]
    }
   ],
   "source": [
    "# 加載模型\n",
    "model = LSTM(\n",
    "    input_dim=X_new_tensor.shape[2],\n",
    "    hidden_dim=best_params[\"hidden_dim\"],\n",
    "    num_layers=best_params[\"num_layers\"],\n",
    "    output_dim=1,\n",
    "    dropout=best_params[\"dropout\"],\n",
    ").to(device)\n",
    "\n",
    "# 加載已保存的模型權重\n",
    "model = torch.load(\"LSTM_model1101.pth\")\n",
    "model.eval()  # 設置為評估模式\n",
    "\n",
    "# 進行預測\n",
    "with torch.no_grad():\n",
    "    prediction = model(X_new_tensor.to(device))\n",
    "    prediction = prediction.cpu().numpy()  # 將結果移回 CPU 並轉為 NumPy 陣列\n",
    "\n",
    "# 反標準化預測結果\n",
    "predicted_price = prediction * y_std + y_mean\n",
    "print(\"Predicted Price per Ping:\", predicted_price[0][0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_scraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
